%!TEX root = ../crowd_hierarchies_v.1.3.tex


\section{Related Work}
\label{sec:related}
The prior work related to the techniques proposed in this paper can be placed in a few categories; we describe each of them in turn:

\vspace{3pt}\noindent\textbf{Crowd Algorithms:} There has been a significant amount of work on designing algorithms where the unit operations (e.g., comparisons, predicate evaluations, and so on) are performed by human workers, including common database primitives such as filter~\cite{crowdscreen}, join~\cite{markus-sorts-joins} and max~\cite{so-who-won},  machine learning primitives such as entity resolution~\cite{entity-matching, crowder} and clustering~\cite{crowdclustering}, as well as data mining primitives~\cite{amsterdamer:2013, get-another-label}. 

Previous work on the task of crowdsourced extraction or enumeration, i.e., populating a database with entities using the crowd~\cite{park:2014, trushkowsky:2013} is the most related to ours. In both cases, the focus is on a single entity extraction query; extracting entities from large and diverse data domains is not considered. Moreover, the proposed techniques do not support  dynamic adaptation of the queries issued against the crowd to optimize for a specified monetary budget. 

%Finally, to optimize the tradeoff between the gain and cost of queries  previous work proposes either a {\em pay-as-you-go} scheme~\cite{trushkowsky:2013} or a fixed answer size scheme~\cite{park:2014}. In the first case, one repeatedly issues queries to the crowd until the {\em marginal gain}, i.e., the difference between the new extracted entities and the querying cost, drops below a desired threshold. However, the proposed scheme does not enforce any budget constraints explicitly and focuses on a single query in isolation. Thus, it does not optimize the gain-cost tradeoff over an entire querying policy. In the second case, one repeatedly issues queries to the crowd until a desired number of entities is retrieved. The latter is specified by the user. Tthis assumes knowledge of the number of entities to be extracted, which may not be available in many real-world scenarios. 

\vspace{3pt}\noindent\textbf{Knowledge Acquisition Systems:} Recent work has also considered the problem of using crowdsourcing within knowledge acquisition systems~\cite{jiang:13, kondredi:2014, west:2014}. This line of work suggests using the crowd for curating knowledge bases (e.g., assessing the validity of the extracted facts) and for gathering additional information to be added to the knowledge base (e.g., missing attributes of an entity or relationships between entities), instead of augmenting the set of entities themselves. As a result, these papers are solving an orthogonal problem. The techniques described in this paper for estimating the amount of information from a query and devising querying strategies to maximize the amount of extracted information will surely be beneficial for knowledge extraction systems as well.

\vspace{3pt}\noindent\textbf{Deep Web Crawling:} A different line of work has focused on data extraction from the deep web~\cite{Jin:2011,Sheng:2012}. In such scenarios, data is obtained by querying a form-based interface over a hidden database and extracting results from the resulting dynamically-generated answer (often a list of entities). Typically, such interfaces provide partial list of matching entities to issued queries; the list is usually limited to the top-k tuples based on an unknown ranking function. Sheng et al.~\cite{Sheng:2012} provide near-optimal algorithms that exploit the exposed structure of the underlying domain to extract all the tuples present in the hidden database under consideration. Our work is similar to this work in that our goal is to also extract entities via a collection of interfaces (in our case the interfaces correspond to queries asked to the crowd).

The main difference between this line of work and ours is that answers from a hidden database are deterministic, i.e., a query in their setting will always retrieve the same top-k tuples. This assumption does not hold in the crowdsourcing scenario considered in this paper and thus the proposed techniques are not applicable. In their setting, it suffices to ask each query precisely once. In our setting, since crowdsourced entity extraction queries can be viewed as random samples from an unknown  distribution, one needs to make use of the query result estimation techniques introduced in \Cref{sec:gainestimators}.